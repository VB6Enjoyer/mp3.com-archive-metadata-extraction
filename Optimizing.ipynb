{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VUevVd5qAumL"
      },
      "outputs": [],
      "source": [
        "# Original\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import re\n",
        "import timeit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original\n",
        "def extract_urls(url, filter_artists=False):\n",
        "    urls = set();  # Use a set to avoid duplicates\n",
        "\n",
        "    response = requests.get(url) # Sends an HTTP GET request to the provided URL and stores the response.\n",
        "    if response.status_code == 200: # If the response status code is 200 (indicating success), it creates a BeautifulSoup object to parse the HTML content.\n",
        "        soup = BeautifulSoup(response.content, 'html.parser');\n",
        "\n",
        "        for a_tag in soup.find_all('a', href=True): # Iterates through all `a` tags with an `href` attribute, extracting the `href` values.\n",
        "            href = a_tag['href'];\n",
        "            full_url = urljoin(url, href); # Join the URL if it's a relative link\n",
        "\n",
        "            if filter_artists:\n",
        "                # Regex pattern for artist pages URLs only\n",
        "                pattern = r\"http://mp3-2003\\.computer-legacy\\.com/artists/\\d+/.+\\.html\"\n",
        "\n",
        "                if re.match(pattern, full_url): # Checks if the URL matches the specified pattern using regex.\n",
        "                    urls.add(full_url);\n",
        "            else:\n",
        "                urls.add(full_url);\n",
        "    return list(urls);"
      ],
      "metadata": {
        "id": "DO4y9vMxA4LH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizado\n",
        "def extract_urls_optimized(url, filter_artists=False):\n",
        "    session = requests.Session()\n",
        "    urls = set()  # Use a set to avoid duplicates\n",
        "\n",
        "    response = session.get(url)  # Use the session for making the request\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Compile the regular expression pattern if needed\n",
        "        if filter_artists:\n",
        "            pattern = re.compile(r\"http://mp3-2003\\.computer-legacy\\.com/artists/\\d+/.+\\.html\")\n",
        "\n",
        "        # Use list comprehension for URL extraction\n",
        "        urls = {\n",
        "            urljoin(url, a_tag['href'])\n",
        "            for a_tag in soup.find_all('a', href=True)\n",
        "            if not filter_artists or (filter_artists and pattern.match(a_tag['href']))\n",
        "        }\n",
        "\n",
        "    return list(urls)"
      ],
      "metadata": {
        "id": "SZsZQ46kBGVI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "website_url = \"http://mp3-2003.computer-legacy.com/artists/browse-09.html\"\n",
        "stmt_original = \"extract_urls(website_url, True)\"\n",
        "setup_original = \"from __main__ import extract_urls, website_url\"\n",
        "\n",
        "execution_time_original = timeit.timeit(stmt_original, setup_original, number=100)\n",
        "\n",
        "print(f\"Execution time for original code: {execution_time_original} seconds\")\n",
        "\n",
        "# Cell 4: Measure Execution Time for Optimized Code\n",
        "\n",
        "stmt_optimized = \"extract_urls_optimized(website_url, True)\"\n",
        "setup_optimized = \"from __main__ import extract_urls_optimized, website_url\"\n",
        "\n",
        "execution_time_optimized = timeit.timeit(stmt_optimized, setup_optimized, number=100)\n",
        "\n",
        "print(f\"Execution time for optimized code: {execution_time_optimized} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VavUrwtdBU4k",
        "outputId": "d5fe5619-d27d-4d6e-ce36-dbe57072fb73"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time for original code: 242.51905584000002 seconds\n",
            "Execution time for optimized code: 235.11716856099997 seconds\n"
          ]
        }
      ]
    }
  ]
}